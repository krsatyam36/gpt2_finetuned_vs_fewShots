{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##IMPORTING NECCESSARY LIBRARIES"
      ],
      "metadata": {
        "id": "abh0YsaarVSb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWQ0zki7qyUU"
      },
      "outputs": [],
      "source": [
        "# Install required packages (first cell in Colab)\n",
        "!pip install -q transformers datasets accelerate evaluate sentencepiece\n",
        "# If you need a specific torch version, install it explicitly (only if necessary)\n",
        "# !pip install -q torch==1.13.1+cu117 -f https://download.pytorch.org/whl/torch_stable.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##IMPORTS AND GPU CHECKS"
      ],
      "metadata": {
        "id": "x2bcxzCPrcVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First I am importing all the necessary libraries that I will use in this notebook.\n",
        "# os, math, and random are standard Python libraries.\n",
        "# pprint is just for printing things in a more readable format.\n",
        "# torch is the PyTorch library which is the backend for Hugging Face models.\n",
        "\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "from pprint import pprint\n",
        "import torch\n",
        "\n",
        "# From the Hugging Face transformers library,\n",
        "# I am importing the GPT-2 model, its tokenizer,\n",
        "# a collator for data batching, the Trainer API for training,\n",
        "# the TrainingArguments class to configure training,\n",
        "# and set_seed for reproducibility.\n",
        "\n",
        "from transformers import (\n",
        "    GPT2LMHeadModel,            # the GPT-2 model with a language modeling head\n",
        "    GPT2TokenizerFast,          # the GPT-2 tokenizer\n",
        "    DataCollatorForLanguageModeling,  # batches text data into the right shape for training\n",
        "    Trainer,                    # high-level training/evaluation loop\n",
        "    TrainingArguments,          # to pass all training hyperparameters\n",
        "    set_seed                    # to make results reproducible\n",
        ")\n",
        "\n",
        "# From the datasets library, I import load_dataset to easily load AG News dataset.\n",
        "# From evaluate, I import evaluation metrics like BLEU.\n",
        "\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "\n",
        "# Now I am setting the random seed. This makes sure that every time I run the notebook,\n",
        "# I get the same results (important for reproducibility).\n",
        "SEED = 42\n",
        "set_seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Next I check if I have a GPU available (like the A100 I am using in Colab).\n",
        "# If I do, I set the device to GPU, otherwise fallback to CPU.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"Device:\", device)\n",
        "if device.type == \"cuda\":\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))   # prints the GPU name if available\n"
      ],
      "metadata": {
        "id": "WEYTmDTerdSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LOADING THE DATASET and THE GPT2 MODEL"
      ],
      "metadata": {
        "id": "EupP1kEkt2Sl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now I am loading the dataset.\n",
        "# I chose the AG News dataset because it has a lot of clean news text data,\n",
        "# which works really well for language modeling experiments.\n",
        "# Hugging Face datasets library makes it super easy to load it with just one line.\n",
        "\n",
        "raw = load_dataset(\"ag_news\")\n",
        "\n",
        "# The dataset comes with different splits (train and test).\n",
        "# So I print the keys to check what splits are available.\n",
        "print(\"Dataset splits: \", raw.keys())\n",
        "\n",
        "# To get a feel of the dataset, I print one example record from the training set.\n",
        "# pprint just makes it easier to read the output in a nice formatted way.\n",
        "pprint(raw[\"train\"][0])\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "cGixVE0vuElN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing train/validation split and keep only text"
      ],
      "metadata": {
        "id": "iaGYDlGEuhqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The AG News dataset actually has both \"text\" and \"label\" columns.\n",
        "# But since I am training a language model, I donâ€™t need the labels (categories).\n",
        "# I only care about the raw text data.\n",
        "# So here, I am removing the \"label\" column from both train and test sets.\n",
        "\n",
        "train_ds = raw[\"train\"].remove_columns(\"label\")\n",
        "val_ds = raw[\"test\"].remove_columns(\"label\")\n",
        "\n",
        "# Now I want to quickly check how big my datasets are.\n",
        "# This prints the number of training and validation samples.\n",
        "print(\"Train size:\", len(train_ds),\n",
        "      \"Validation size:\", len(val_ds))\n",
        "\n",
        "# Finally, I print one example text just to make sure everything looks correct.\n",
        "print(\"Example Text:\", train_ds[0][\"text\"])\n"
      ],
      "metadata": {
        "id": "sJ4vWFO5wHri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
        "\n",
        "# First, I choose the base GPT-2 model.\n",
        "# \"gpt2\" here refers to the small GPT-2 (124M parameters), which is lightweight and Colab-friendly, can also use the GPT-2 medium, or large, but using GPT-2 for the time being.\n",
        "MODEL_NAME = \"gpt2\"\n",
        "\n",
        "# Now I load the tokenizer for GPT-2.\n",
        "# A tokenizer converts raw text into numbers (token IDs) that the model can understand.\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# GPT-2 does not originally come with a pad token.\n",
        "# Since we need padding for batching, I set the pad token to be the same as the end-of-sequence (EOS) token.\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Next, I load the GPT-2 model itself.\n",
        "# GPT2LMHeadModel is the GPT-2 model with a \"language modeling head\" on top,\n",
        "# which makes it suitable for text generation and autoregressive training.\n",
        "base_model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Because I added a pad token above, I need to resize the model's embeddings\n",
        "# so the model knows about the new token.\n",
        "base_model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# setting the pad_token_id in the model's config, so it wonâ€™t get confused while training.\n",
        "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "#  moving the model to GPU (if available) so training/inference is much faster.\n",
        "base_model.to(device)\n"
      ],
      "metadata": {
        "id": "rU0IjWgJwxIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TOKENIZEING THE DATASET"
      ],
      "metadata": {
        "id": "gDIYnrOlxxEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, I define a helper function to tokenize the text.\n",
        "# It takes the \"text\" field from the dataset and converts it into token IDs using the GPT-2 tokenizer.\n",
        "# I also enable truncation so that very long texts donâ€™t break the tokenizer.\n",
        "def tokenize_batch(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True)\n",
        "\n",
        "# Now I apply this tokenizer function to both training and validation datasets.\n",
        "# \"batched=True\" means it will process multiple examples at once (faster).\n",
        "# I also remove the original raw text column since I only need the tokenized version.\n",
        "tokenized_train = train_ds.map(tokenize_batch, batched=True, remove_columns=train_ds.column_names)\n",
        "tokenized_val = val_ds.map(tokenize_batch, batched=True, remove_columns=val_ds.column_names)\n",
        "\n",
        "# Next, I set the block size (context length) for training.\n",
        "# Since I am using an A100 GPU with large memory, I can safely use 512 tokens.\n",
        "block_size = 512\n",
        "\n",
        "# This function groups the tokenized text into fixed-size blocks of length = block_size.\n",
        "# Basically, I take all the token IDs, flatten them, and then split them into equal chunks of 512.\n",
        "# This ensures the model always sees consistent input lengths during training.\n",
        "def group_texts(examples):\n",
        "    all_ids = sum(examples[\"input_ids\"], [])   # flatten into a single list\n",
        "    total_len = (len(all_ids) // block_size) * block_size  # drop leftover tokens that donâ€™t fit\n",
        "    chunks = [all_ids[i:i+block_size] for i in range(0, total_len, block_size)]\n",
        "    return {\n",
        "        \"input_ids\": chunks,\n",
        "        \"attention_mask\": [[1]*block_size for _ in range(len(chunks))]  # mask for attention\n",
        "    }\n",
        "\n",
        "# Now I apply the grouping function to both train and validation sets.\n",
        "# Again, \"batched=True\" so it handles multiple samples efficiently.\n",
        "# I set batch_size=1000 here to balance speed and memory usage.\n",
        "lm_train = tokenized_train.map(group_texts, batched=True, batch_size=1000)\n",
        "lm_val = tokenized_val.map(group_texts, batched=True, batch_size=1000)\n",
        "\n",
        "# cheking how many training/validation blocks we ended up with.\n",
        "print(\"Training blocks:\", len(lm_train))\n",
        "print(\"Validation blocks:\", len(lm_val))\n"
      ],
      "metadata": {
        "id": "FpyTteTSx17F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##FEW SHOT PROMPTING (BASELINE GPT2, BEFORE FINE-TUNING)"
      ],
      "metadata": {
        "id": "vRQqcR-PzGPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function builds a prompt with k examples from the dataset.\n",
        "# For example:\n",
        "#   k=0 â†’ zero-shot (no examples, just the query).\n",
        "#   k=1 â†’ one-shot (one example + the query).\n",
        "#   k=3 â†’ few-shot (three examples + the query).\n",
        "# I concatenate k examples into a single text prompt.\n",
        "def build_prompt(k, dataset, idx=0):\n",
        "    chosen = dataset.select(range(k+1))  # take k+1 samples\n",
        "    prompt = \"\"\n",
        "    for i in range(k):\n",
        "        prompt += f\"News: {chosen[i]['text']}\\n\\n\"  # add example news texts\n",
        "    prompt += f\"News: {chosen[k]['text']}\\n\"        # final \"query\" example\n",
        "    return prompt\n",
        "\n",
        "# This function generates text given a prompt.\n",
        "# Steps:\n",
        "# 1. Tokenize the prompt into input_ids.\n",
        "# 2. Use model.generate() to predict new tokens.\n",
        "# 3. Decode the generated tokens back into text.\n",
        "def generate_text(model, tokenizer, prompt, max_new_tokens=40):\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "    gen_ids = model.generate(\n",
        "        input_ids,\n",
        "        do_sample=True,                 # enables sampling instead of greedy decoding\n",
        "        top_p=0.9,                      # nucleus sampling: only sample from top 90% probability mass\n",
        "        temperature=0.7,                # controls randomness (lower = more focused)\n",
        "        max_new_tokens=max_new_tokens,  # how many new tokens to generate\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    gen_text = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
        "    # Remove the original prompt from the output, keeping only the model's continuation.\n",
        "    return gen_text[len(prompt):].strip()\n",
        "\n",
        "# testing the pretrained GPT-2 (before fine-tuning) with k = 0, 1, 3.\n",
        "# This shows how well the base GPT-2 can handle few-shot learning directly.\n",
        "for k in [0, 1, 3]:\n",
        "    prompt = build_prompt(k, val_ds, idx=5)  # pick index 5 from validation set\n",
        "    gen_output = generate_text(base_model, tokenizer, prompt)\n",
        "\n",
        "    print(\"=\"*100)\n",
        "    print(f\" FEW-SHOT TEST (k={k})\")\n",
        "    print(\"=\"*100)\n",
        "\n",
        "    print(\"\\nPrompt given to model:\\n\")\n",
        "    print(prompt.strip())\n",
        "\n",
        "    print(\"\\nModel Generated:\\n\")\n",
        "    print(gen_output.strip())\n",
        "    print(\"\\n\\n\")\n"
      ],
      "metadata": {
        "id": "RvqqglHLzO9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "INTERACTIVE PROMTING"
      ],
      "metadata": {
        "id": "rAEPuOMB6Ltl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function is my custom tester for few-shot learning.\n",
        "# Instead of always relying on the dataset, I can pass my own list of \"news\" examples.\n",
        "# The function builds a prompt, feeds it into the model, and prints the continuation.\n",
        "\n",
        "def test_few_shot(model, tokenizer, prompt_texts, max_new_tokens=50):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "      model: The GPT model (can be base GPT-2 or fine-tuned version).\n",
        "      tokenizer: The GPT tokenizer to convert text into tokens.\n",
        "      prompt_texts: A list of custom news strings, like:\n",
        "                    [\"News: ...\", \"News: ...\"].\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Build the full prompt by joining all the news examples together.\n",
        "    # I separate them with blank lines for readability.\n",
        "    prompt = \"\\n\\n\".join([f\"News: {t}\" for t in prompt_texts])\n",
        "\n",
        "    # Step 2: Tokenize the prompt into input IDs and attention mask.\n",
        "    # This prepares the text in a format that the model can actually process.\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    attn_mask = inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "    # Step 3: Generate continuation from the model.\n",
        "    # I use sampling here (not greedy decoding), so results are more diverse.\n",
        "    gen_ids = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attn_mask,\n",
        "        do_sample=True,          # sample from probability distribution\n",
        "        top_p=0.9,               # nucleus sampling (take top 90% probability mass)\n",
        "        temperature=0.7,         # controls randomness (lower = more deterministic)\n",
        "        max_new_tokens=max_new_tokens,  # number of new tokens to generate\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        repetition_penalty=1.2,  # avoid repeating the same phrase too much\n",
        "        min_new_tokens=40        # make sure at least 40 tokens are generated\n",
        "    )\n",
        "\n",
        "    # Step 4: Decode the tokens back into human-readable text.\n",
        "    gen_text = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Step 5: Print everything nicely formatted.\n",
        "    print(\"=\"*100)\n",
        "    print(\"Prompt given:\\n\")\n",
        "    print(prompt)\n",
        "    print(\"\\nModel continuation:\\n\")\n",
        "    print(gen_text[len(prompt):].strip())  # remove the prompt, keep only new content\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# ðŸ”¹ Example usage of the function with my own prompts.\n",
        "custom_examples = [\n",
        "    \"AI startup raises $50M to build next-gen language model\",\n",
        "    \"New vaccine shows promise in clinical trials\",\n",
        "    \"Stock markets rally after positive earnings reports\",\n",
        "]\n",
        "\n",
        "# Here I test the base GPT-2 (before fine-tuning) with my custom examples.\n",
        "test_few_shot(base_model, tokenizer, custom_examples)\n"
      ],
      "metadata": {
        "id": "wPBTNuUN2vnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##FINETUNING GPT2 ON AG NEWS DATASET"
      ],
      "metadata": {
        "id": "C0tRau8Q76hE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments"
      ],
      "metadata": {
        "id": "zPvhs3fW7_45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First, I remove any old fine-tuned model checkpoints\n",
        "# so that I always start fresh without conflicts.\n",
        "!rm -rf ./gpt2-finetuned-agnews\n",
        "\n",
        "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments, GPT2LMHeadModel\n",
        "\n",
        "# I reload a fresh GPT-2 base model (same as before).\n",
        "# This way, I fine-tune from scratch (on AG News) rather than continuing a broken run.\n",
        "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
        "model.resize_token_embeddings(len(tokenizer))   # adjust embedding layer in case pad token was added\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "model.to(device)  # push to GPU\n",
        "\n",
        "# The data collator is responsible for batching data during training.\n",
        "# Since I am training a language model (not MLM like BERT),\n",
        "# I set mlm=False â†’ this means we use *causal LM objective* (predict next token).\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# Here I define the training arguments.\n",
        "# These are the knobs that control how training happens.\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-finetuned-agnews\",   # save checkpoints to this folder\n",
        "    overwrite_output_dir=True,              # overwrite if folder already exists\n",
        "    num_train_epochs=20,                    # I train for 20 epochs\n",
        "    per_device_train_batch_size=64,         # batch size per GPU\n",
        "    gradient_accumulation_steps=4,          # accumulate gradients â†’ effective batch size = 64*4=256\n",
        "    save_strategy=\"epoch\",                  # save checkpoint at the end of every epoch\n",
        "    logging_steps=100,                      # log loss every 100 steps\n",
        "    learning_rate=3e-5,                     # learning rate for AdamW optimizer\n",
        "    weight_decay=0.01,                      # small L2 regularization to prevent overfitting\n",
        "    fp16=torch.cuda.is_available(),         # use half-precision if GPU supports it (faster, less memory)\n",
        "    report_to=\"none\",                       # I donâ€™t log to WandB/Hub, just local training\n",
        "    save_total_limit=2                      # keep only the last 2 checkpoints to save space\n",
        ")\n",
        "\n",
        "# Now I build the HuggingFace Trainer.\n",
        "# This wraps the model, data, and training args all together.\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=lm_train,\n",
        "    eval_dataset=lm_val,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Finally, I launch the training process.\n",
        "# This is where fine-tuning actually happens ðŸš€.\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "qWY_M1EM9a7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "aiming for a better train_loss\n",
        "removing the checkpoint 570 and 665 dirs\n",
        "7 epochs at the above given params gave train_loss 3.26 avg\n",
        "\n",
        "\n",
        "in 20 epochs, the train_loss is 3.23 a little better than 7 epochs"
      ],
      "metadata": {
        "id": "Aij4i47pH9mN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ./gpt2-finetuned-agnews\n"
      ],
      "metadata": {
        "id": "T6sWwMY6IF-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PERPLEXITY EVALATION (HOW SUPRRISED THE MODOEL IS AFTER SEEING NEW WORDS)"
      ],
      "metadata": {
        "id": "tibLY5Ry6SUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "eval_results = trainer.evaluate()\n",
        "eval_loss = eval_results[\"eval_loss\"]\n",
        "perplexity = math.exp(eval_loss)\n",
        "print(f\"Validation loss: {eval_loss:.4f} â†’ Perplexity: {perplexity:.2f}\")\n"
      ],
      "metadata": {
        "id": "0TMy0fFH6bch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ft_model = GPT2LMHeadModel.from_pretrained(\"./gpt2-finetuned-agnews/checkpoint-960\").to(device)\n",
        "trainer.save_model(\"./gpt2-finetuned-agnews-final\")\n",
        "ft_model = GPT2LMHeadModel.from_pretrained(\"./gpt2-finetuned-agnews-final\").to(device)\n"
      ],
      "metadata": {
        "id": "XBptMs2WLCeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_idx = 10  # you can change this index to try different samples\n",
        "for k in [0, 1, 3]:\n",
        "    prompt = build_prompt(k, val_ds, idx=test_idx)\n",
        "    base_out = generate_text(base_model, tokenizer, prompt)\n",
        "    ft_out = generate_text(ft_model, tokenizer, prompt)\n",
        "\n",
        "    print(\"=\"*100)\n",
        "    print(f\"Prompt (K={k}):\\n{prompt[:300]}...\\n\")\n",
        "    print(\"Pretrained GPT-2 â†’\", base_out, \"\\n\")\n",
        "    print(\"Fine-tuned GPT-2 â†’\", ft_out)\n",
        "    print(\"=\"*100, \"\\n\")\n"
      ],
      "metadata": {
        "id": "K399dJrMLmmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##BLEU SCORE EVALUATION"
      ],
      "metadata": {
        "id": "NjsRndjkMA-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "\n",
        "def evaluate_bleu(model, tokenizer, dataset, k=0, num_samples=100):\n",
        "    preds, refs = [], []\n",
        "    for i in range(num_samples):\n",
        "        prompt = build_prompt(k, dataset, idx=i)\n",
        "        gen = generate_text(model, tokenizer, prompt)\n",
        "        preds.append(gen)\n",
        "        refs.append([dataset[i][\"text\"]])  # reference is the real news text\n",
        "    return bleu.compute(predictions=preds, references=refs)\n",
        "\n",
        "print(\"\\nBaseline GPT-2 BLEU:\", evaluate_bleu(base_model, tokenizer, val_ds, k=0, num_samples=100))\n",
        "print(\"Fine-tuned GPT-2 BLEU:\", evaluate_bleu(ft_model, tokenizer, val_ds, k=0, num_samples=100))\n"
      ],
      "metadata": {
        "id": "LYutEh3AMEmw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
